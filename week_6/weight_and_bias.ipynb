{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases in Neural Networks\n",
    "\n",
    "In neural networks, **weights** and **biases** are core parameters that define how each neuron processes the incoming data and generates outputs. These parameters are learned from data during the training process and are optimized to improve the performance of the network.\n",
    "\n",
    "#### 1. Weights\n",
    "\n",
    "**Definition**:  \n",
    "Weights are numerical values assigned to the connections between neurons in the network. Each input to a neuron has an associated weight, which determines how much influence that particular input will have on the neuron's output.\n",
    "\n",
    "**Purpose**:\n",
    "- **Strength of Connection**: Weights control how strongly an input contributes to the neuron's output. Higher weights give more importance to the corresponding input.\n",
    "- **Learning Representation**: Through weights, the network learns how to best represent the relationship between the input features and the target output.\n",
    "\n",
    "**Where Do We Get Weights From?**\n",
    "- **Initialization**: Weights are initialized with random values at the start of training. These values may be initialized using methods like **Xavier initialization** or **He initialization**.\n",
    "- **Training**: During training, weights are updated through an optimization process (like gradient descent) to minimize the error between predicted and actual outputs.\n",
    "\n",
    "**Example of Weights**:\n",
    "Consider a simple neural network with inputs $ x_1, x_2 $ and weights $ w_1, w_2 $:\n",
    "\n",
    "$z = w_1 \\cdot x_1 + w_2 \\cdot x_2$\n",
    "\n",
    "This weighted sum $ z $ is then passed through an activation function to get the output.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Biases\n",
    "\n",
    "**Definition**:  \n",
    "A bias is a parameter added to the weighted sum before the output is passed through the activation function. It allows the neuron to shift its activation function, providing flexibility in learning complex patterns.\n",
    "\n",
    "**Purpose**:\n",
    "- **Shifting Activation**: The bias helps to shift the activation function to the left or right, enabling the network to model more complex patterns.\n",
    "- **Default Output**: The bias ensures that a neuron can produce a non-zero output even if all input values are zero.\n",
    "\n",
    "**Where Do We Get Biases From?**\n",
    "- **Initialization**: Like weights, biases are typically initialized to small random values or zeros at the beginning of training.\n",
    "- **Training**: Biases are adjusted during training through the same optimization algorithm (e.g., gradient descent).\n",
    "\n",
    "**Example of Bias**:\n",
    "For a single neuron, the weighted sum with bias is:\n",
    "\n",
    "$z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b$\n",
    "\n",
    "This sum $ z $ is then passed through an activation function to get the output.\n",
    "\n",
    "---\n",
    "\n",
    "#### Significance of Weights and Biases\n",
    "\n",
    "- **Learning Function**: The weights allow the network to learn the relationships between inputs and outputs, while the bias ensures the neuron can adapt to various patterns.\n",
    "- **Flexibility**: Weights and biases provide the flexibility needed for neural networks to capture complex patterns and make predictions across a wide variety of tasks.\n",
    "- **Optimization**: Both weights and biases are optimized during training to reduce the error, allowing the network to improve its performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### In Summary\n",
    "Weights and biases are the fundamental components of a neural network. They are initialized with random values and are adjusted during training to minimize the loss function. The **weights** control the strength of the input features, while the **biases** allow the model to shift its activation function and handle more complex patterns. Together, they enable the network to learn from data and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sum (z): 0.78\n",
      "Output after activation: 0.6856801139382539\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the inputs and weights\n",
    "x1 = 0.5  # Input 1\n",
    "x2 = 0.3  # Input 2\n",
    "w1 = 0.8  # Weight for input 1\n",
    "w2 = 0.6  # Weight for input 2\n",
    "b = 0.2   # Bias\n",
    "\n",
    "# Calculate the weighted sum (z)\n",
    "z = w1 * x1 + w2 * x2 + b\n",
    "print(\"Weighted Sum (z):\", z)\n",
    "\n",
    "# Define the activation function (e.g., Sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Pass the weighted sum through the activation function\n",
    "output = sigmoid(z)\n",
    "print(\"Output after activation:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
