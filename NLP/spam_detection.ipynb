{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam messages detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install Required Libraries\n",
    "\n",
    "```bash\n",
    "pip install tensorflow pandas numpy\n",
    "```\n",
    "\n",
    "#### Step 2: Load and Preprocess Data\n",
    "\n",
    "Load the SMS Spam Collection dataset. Here’s the basic code structure for loading the dataset from UCI.\n",
    "\n",
    "**Download the dataset**\n",
    "\n",
    "1. Go to the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) and download `SMSSpamCollection` file.\n",
    "2. Place the file in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "data['label'] = data['label'].map({'ham': 0, 'spam': 1})  # Encode labels as binary\n",
    "print(data.head())\n",
    "\n",
    "# Split into sentences and labels\n",
    "sentences = data['message'].values\n",
    "labels = data['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Preprocess the Text Data\n",
    "\n",
    "We’ll use `tf.keras.layers.TextVectorization` for preprocessing, which helps convert text into a numerical format suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parameters for vectorization\n",
    "max_vocab_size = 1000  # Vocabulary size\n",
    "max_sequence_length = 50  # Maximum length of a sequence\n",
    "\n",
    "# Vectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the vectorization layer on the text data\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Apply the vectorization to the text data\n",
    "vectorized_sentences = vectorize_layer(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Splitting data\n",
    "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(\n",
    "    vectorized_sentences.numpy(), labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Build the Model\n",
    "\n",
    "Define a simple neural network model using TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 16)            16000     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16289 (63.63 KB)\n",
      "Trainable params: 16289 (63.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Building the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab_size, output_dim=16, input_length=max_sequence_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/140 [==============================] - 2s 3ms/step - loss: 0.5624 - accuracy: 0.8450 - val_loss: 0.3849 - val_accuracy: 0.8664\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.8658 - val_loss: 0.2916 - val_accuracy: 0.8664\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.2463 - accuracy: 0.8775 - val_loss: 0.1924 - val_accuracy: 0.9202\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9536 - val_loss: 0.1168 - val_accuracy: 0.9587\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9711 - val_loss: 0.0857 - val_accuracy: 0.9731\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9749 - val_loss: 0.0704 - val_accuracy: 0.9776\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9778 - val_loss: 0.0616 - val_accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9812 - val_loss: 0.0594 - val_accuracy: 0.9839\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9832 - val_loss: 0.0530 - val_accuracy: 0.9857\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9854 - val_loss: 0.0515 - val_accuracy: 0.9865\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    training_sentences, training_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(testing_sentences, testing_labels)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluate the Model\n",
    "\n",
    "After training, we evaluate the model’s performance on the test set to get an idea of how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9865\n",
      "Test Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(testing_sentences, testing_labels)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Make Predictions\n",
    "\n",
    "Finally, we can use the model to make predictions on new text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Prediction: [[0.79941714]]\n",
      "Spam\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Prediction: [[0.00977313]]\n",
      "Ham\n"
     ]
    }
   ],
   "source": [
    "def predict_spam(text):\n",
    "    vectorized_text = vectorize_layer([text])\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    print(f'Prediction: {prediction}')\n",
    "    return \"Spam\" if prediction[0] > 0.5 else \"Ham\"\n",
    "\n",
    "# Test the function\n",
    "print(predict_spam(\"Congratulations! You've won a free iPhone. Claim now!\"))\n",
    "print(predict_spam(\"Let's catch up for lunch tomorrow.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
