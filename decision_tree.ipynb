{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding Decision Trees in Depth**\n",
    "\n",
    "Decision trees are a popular and powerful machine learning algorithm used for both classification and regression tasks. They are intuitive, easy to interpret, and capable of handling both numerical and categorical data. In this detailed exploration, we'll cover the fundamental concepts behind decision trees, how they work, and implement one using dummy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Concepts of Decision Trees\n",
    "#### 1.1 What is a Decision Tree?\n",
    "\n",
    "A decision tree is a flowchart-like structure where:\n",
    "\n",
    "- Each internal node represents a test on a feature (attribute).\n",
    "- Each branch represents the outcome of the test.\n",
    "- Each leaf node (terminal node) represents a class label (in classification) or a continuous value (in regression).\n",
    "\n",
    "#### 1.2 Key Concepts\n",
    "\n",
    "- Entropy: A measure of impurity or disorder in a dataset. The higher the entropy, the more uncertain we are about the class labels.\n",
    "- Information Gain: The reduction in entropy after splitting the dataset based on a feature. The goal is to choose the feature that results in the highest information gain.\n",
    "- Tree Pruning: The process of removing branches from the tree that have little importance, which helps to avoid overfitting.\n",
    "\n",
    "#### 2. How Decision Trees Work\n",
    "\n",
    "The process of building a decision tree can be broken down into the following steps:\n",
    "\n",
    "- Select the Best Feature: At each node, evaluate all features and select the one that maximizes information gain (or minimizes entropy).\n",
    "\n",
    "- Split the Data: Based on the selected feature, split the dataset into subsets.\n",
    "\n",
    "- Repeat: Recursively apply steps 1 and 2 on the subsets until one of the stopping conditions is met:\n",
    "    - All samples in a node belong to the same class.\n",
    "    - No remaining features to split on.\n",
    "    - The maximum depth of the tree has been reached.\n",
    "    - The number of samples in a node is less than a specified threshold.\n",
    "\n",
    "- Assign Class Labels: Once the tree is built, classify new instances by traversing the tree based on feature values.\n",
    "\n",
    "#### 3. Dummy Data Example\n",
    "\n",
    "Let's create a simple dummy dataset to illustrate how a decision tree works. We'll create a dataset that predicts whether a person likes a particular type of fruit based on their age and income level.\n",
    "\n",
    "#### 3.1 Create Dummy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TERMINALOGIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-depth explanation of key concepts such as **entropy**, **information gain**, **Gini index**, and **tree pruning**. \n",
    "\n",
    "### 1. Entropy\n",
    "\n",
    "**Definition**:  \n",
    "Entropy is a measure of uncertainty or impurity in a dataset. In the context of decision trees, it quantifies the amount of disorder or randomness in the target variable (the variable we are trying to predict).\n",
    "\n",
    "**Importance**:\n",
    "- **Impurity Measure**: Helps determine how mixed the classes in a dataset are. A high entropy value indicates a high degree of disorder (e.g., a dataset with an equal distribution of classes), while a low entropy value indicates a more ordered dataset.\n",
    "- **Information Gain Calculation**: Used to calculate information gain, which helps in selecting the best features to split the data at each node of the tree.\n",
    "\n",
    "**How It Works**:  \n",
    "The formula for entropy \\( H \\) of a dataset is given by:\n",
    "\n",
    "\n",
    "\n",
    "#### Example:\n",
    "Consider a binary classification problem with a dataset of 10 instances where:\n",
    "- 6 instances are of Class A\n",
    "- 4 instances are of Class B\n",
    "\n",
    "The proportions would be:\n",
    "- \\( p_A = \\frac{6}{10} = 0.6 \\)\n",
    "- \\( p_B = \\frac{4}{10} = 0.4 \\)\n",
    "\n",
    "The entropy of this dataset would be calculated as follows:\n",
    "\n",
    "\\[\n",
    "H(S) = -\\left(0.6 \\log_2 0.6 + 0.4 \\log_2 0.4\\right) \\approx 0.970\n",
    "\\]\n",
    "\n",
    "### 2. Information Gain\n",
    "\n",
    "**Definition**:  \n",
    "Information gain measures the reduction in entropy after a dataset is split on an attribute. It quantifies how much knowing the value of an attribute improves our prediction of the target variable.\n",
    "\n",
    "**Importance**:  \n",
    "- **Feature Selection**: Helps in deciding which feature to split the data on at each node of the tree. The feature with the highest information gain is selected to create the branches.\n",
    "\n",
    "#### Example:\n",
    "Continuing from the previous dataset, let's say we split the data based on another feature that divides it into two groups:\n",
    "\n",
    "1. Group 1: 4 instances of Class A, 1 instance of Class B (Entropy = 0.321)\n",
    "2. Group 2: 2 instances of Class A, 3 instances of Class B (Entropy = 0.971)\n",
    "\n",
    "### 3. Tree Pruning\n",
    "\n",
    "**Definition**:  \n",
    "Pruning involves removing nodes from a decision tree that provide little power in classifying instances, thereby reducing the complexity of the model.\n",
    "\n",
    "**Importance**:  \n",
    "- **Overfitting Prevention**: Pruning helps to prevent overfitting, where the model becomes too complex and performs poorly on unseen data.\n",
    "\n",
    "**How It Works**:  \n",
    "There are two types of pruning:\n",
    "- **Pre-pruning**: Stops the tree from growing once it meets certain criteria (e.g., a minimum number of samples in a node).\n",
    "- **Post-pruning**: Grows the full tree and then removes nodes that do not provide significant predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income Likes_Fruit\n",
      "0   22   50000         Yes\n",
      "1   25   60000         Yes\n",
      "2   47  120000          No\n",
      "3   35   80000         Yes\n",
      "4   32   75000          No\n",
      "5   26   62000         Yes\n",
      "6   60   30000          No\n",
      "7   45   90000         Yes\n",
      "8   41   70000          No\n",
      "9   30   55000         Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dummy dataset\n",
    "data = {\n",
    "    'Age': [22, 25, 47, 35, 32, 26, 60, 45, 41, 30],\n",
    "    'Income': [50000, 60000, 120000, 80000, 75000, 62000, 30000, 90000, 70000, 55000],\n",
    "    'Likes_Fruit': ['Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Step-by-Step Decision Tree Implementation\n",
    "\n",
    "We will manually build a decision tree based on our dummy dataset.\n",
    "#### 4.1 Calculate Entropy\n",
    "\n",
    "To begin, we need to calculate the initial entropy of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Entropy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        p_label = np.sum(y == label) / len(y)\n",
    "        entropy -= p_label * np.log2(p_label) if p_label > 0 else 0\n",
    "    return entropy\n",
    "\n",
    "# Calculate the initial entropy\n",
    "initial_entropy = calculate_entropy(df['Likes_Fruit'])\n",
    "print(f'Initial Entropy: {initial_entropy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Information Gain Calculation\n",
    "\n",
    "Next, we need to calculate the information gain for each feature by splitting the dataset on each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for Age: 0.2564\n",
      "Information Gain for Income: 0.0200\n"
     ]
    }
   ],
   "source": [
    "def information_gain(y, x_feature, threshold):\n",
    "    parent_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Create masks for left and right splits\n",
    "    left_mask = x_feature <= threshold\n",
    "    right_mask = x_feature > threshold\n",
    "    n = len(y)\n",
    "\n",
    "    # Check if splits are possible\n",
    "    if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "        return 0  # No split possible\n",
    "\n",
    "    n_left, n_right = np.sum(left_mask), np.sum(right_mask)\n",
    "    \n",
    "    # Calculate child entropy\n",
    "    child_entropy = (n_left / n) * calculate_entropy(y[left_mask]) + (n_right / n) * calculate_entropy(y[right_mask])\n",
    "    \n",
    "    # Calculate Information Gain\n",
    "    return parent_entropy - child_entropy\n",
    "\n",
    "# Calculate information gain for Age and Income\n",
    "info_gain_age = information_gain(df['Likes_Fruit'], df['Age'], 35)\n",
    "info_gain_income = information_gain(df['Likes_Fruit'], df['Income'], 70000)\n",
    "\n",
    "print(f'Information Gain for Age: {info_gain_age:.4f}')\n",
    "print(f'Information Gain for Income: {info_gain_income:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building the Tree\n",
    "\n",
    "Based on the information gain, we can decide which feature to split on. In this case, we would choose Age since it has a higher information gain. We can recursively repeat this process until we reach the stopping criteria.\n",
    "\n",
    "Here's how we would implement this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, output=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.output = output\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3):\n",
    "    # Check stopping conditions\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return DecisionTreeNode(output=np.unique(y)[0])\n",
    "    if depth >= max_depth:\n",
    "        return DecisionTreeNode(output=np.bincount(y).argmax())\n",
    "\n",
    "    best_feature, best_threshold, best_gain = None, None, -1\n",
    "\n",
    "    for feature in ['Age', 'Income']:\n",
    "        thresholds = np.unique(X[feature])\n",
    "        for threshold in thresholds:\n",
    "            gain = information_gain(y, X[feature], threshold)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "    if best_gain == 0:\n",
    "        return DecisionTreeNode(output=np.bincount(y).argmax())\n",
    "\n",
    "    left_mask = X[best_feature] <= best_threshold\n",
    "    right_mask = X[best_feature] > best_threshold\n",
    "\n",
    "    left_child = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth)\n",
    "    right_child = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth)\n",
    "\n",
    "    return DecisionTreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "# Build the decision tree\n",
    "tree = build_tree(df[['Age', 'Income']], df['Likes_Fruit'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Making Predictions\n",
    "\n",
    "Now that we have built our decision tree, we can use it to make predictions for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for {'Age': 28, 'Income': 65000}: Yes\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, x):\n",
    "    if tree.output is not None:\n",
    "        return tree.output\n",
    "    if x[tree.feature] <= tree.threshold:\n",
    "        return predict(tree.left, x)\n",
    "    else:\n",
    "        return predict(tree.right, x)\n",
    "\n",
    "# Test prediction\n",
    "test_data = {'Age': 28, 'Income': 65000}\n",
    "prediction = predict(tree, test_data)\n",
    "print(f'Prediction for {test_data}: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- Age <= 31.00\n",
      "|   |--- class: Yes\n",
      "|--- Age >  31.00\n",
      "|   |--- Income <= 77500.00\n",
      "|   |   |--- class: No\n",
      "|   |--- Income >  77500.00\n",
      "|   |   |--- Income <= 105000.00\n",
      "|   |   |   |--- class: Yes\n",
      "|   |   |--- Income >  105000.00\n",
      "|   |   |   |--- class: No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Preparing the data\n",
    "X = df[['Age', 'Income']]\n",
    "y = df['Likes_Fruit']\n",
    "\n",
    "# Fit the Decision Tree Classifier\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Visualize the tree\n",
    "tree_rules = export_text(classifier, feature_names=list(X.columns))\n",
    "print(tree_rules)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
