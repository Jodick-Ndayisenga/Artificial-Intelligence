{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Backpropagation â€“ Learning from Mistakes\n",
    "\n",
    "## Overview\n",
    "In this session, we explore **backpropagation**, the core algorithm for training neural networks. Backpropagation enables the model to adjust its weights by learning from its errors, making it crucial for improving neural network performance.\n",
    "\n",
    "Backpropagation uses the **chain rule of calculus** to compute gradients, allowing the network to update weights during training. By minimizing the error between the predicted and actual values, the network's accuracy improves over time.\n",
    "\n",
    "## Key Topics\n",
    "- Understanding Backpropagation\n",
    "- The Chain Rule and Gradient Calculation\n",
    "- Backpropagation Algorithm\n",
    "- How Backpropagation Updates Weights\n",
    "- Practical Implementation of Backpropagation\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Understanding Backpropagation\n",
    "\n",
    "Backpropagation is a supervised learning technique that optimizes neural network weights. It propagates error backwards from the output layer to the input layer, adjusting weights accordingly.\n",
    "\n",
    "In a neural network:\n",
    "- The **forward pass** calculates output using input and current weights.\n",
    "- The **backward pass** adjusts weights by calculating the gradient of the loss function with respect to each weight.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Chain Rule and Gradient Calculation\n",
    "\n",
    "The gradient measures how much the output changes with respect to input changes. In machine learning, gradients help adjust weights during backpropagation.\n",
    "\n",
    "The **chain rule** allows us to compute the derivative of a composite function. Backpropagation applies the chain rule to compute the derivative of the loss function with respect to each weight.\n",
    "\n",
    "#### Chain Rule Explanation\n",
    "If the loss function \\( L \\) depends on an intermediate variable \\( z \\), and \\( z \\) depends on \\( w \\), the chain rule states:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$\n",
    "\n",
    "Backpropagation uses this iteratively across each layer to update weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Backpropagation Algorithm\n",
    "\n",
    "**Backpropagation** follows these steps:\n",
    "\n",
    "1. **Forward Pass**: Input is fed through the network to calculate output.\n",
    "2. **Backward Pass**: Starting from the output layer, compute gradients by applying the chain rule.\n",
    "3. **Weight Update**: Adjust weights using gradient descent.\n",
    "4. **Repeat**: Iterate to minimize loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How Backpropagation Updates Weights\n",
    "\n",
    "Backpropagation uses **gradient descent** to update weights with this rule:\n",
    "\n",
    "$w = w - \\text{learning rate} \\times \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "Where:\n",
    "- $ \\frac{\\partial L}{\\partial w} $: Gradient of the loss with respect to the weight.\n",
    "- **Learning rate**: Controls step size in adjusting weights.\n",
    "\n",
    "#### Example of Weight Updates:\n",
    "Given a neural network with one hidden layer:\n",
    "1. Calculate hidden layer output $ h = f(W \\cdot x + b) $.\n",
    "2. Output layer output $ y = f(W' \\cdot h + b') $.\n",
    "3. Calculate loss $ L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2 $.\n",
    "4. Compute and propagate gradients to adjust weights.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Practical Implementation of Backpropagation\n",
    "\n",
    "The following Python example demonstrates backpropagation on a simple neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Backpropagation for a Simple Neural Network\n",
    "This code demonstrates backpropagation on a neural network solving the XOR problem.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Neural Network structure\n",
    "input_layer_size = 3\n",
    "hidden_layer_size = 4\n",
    "output_layer_size = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "W1 = np.random.rand(input_layer_size, hidden_layer_size)\n",
    "b1 = np.random.rand(1, hidden_layer_size)\n",
    "W2 = np.random.rand(hidden_layer_size, output_layer_size)\n",
    "b2 = np.random.rand(1, output_layer_size)\n",
    "\n",
    "# Input data and target output\n",
    "X = np.array([[0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1]])  # 4 examples, 3 features\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR problem\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "iterations = 10000\n",
    "\n",
    "# Training loop\n",
    "for i in range(iterations):\n",
    "    # Forward pass\n",
    "    hidden_layer_input = np.dot(X, W1) + b1\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    \n",
    "    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = np.mean((output_layer_output - y) ** 2)\n",
    "    \n",
    "    # Backward pass\n",
    "    output_layer_error = output_layer_output - y\n",
    "    output_layer_gradient = output_layer_error * sigmoid_derivative(output_layer_output)\n",
    "    \n",
    "    hidden_layer_error = output_layer_gradient.dot(W2.T)\n",
    "    hidden_layer_gradient = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # Update weights and biases using gradient descent\n",
    "    W2 -= learning_rate * hidden_layer_output.T.dot(output_layer_gradient)\n",
    "    b2 -= learning_rate * np.sum(output_layer_gradient, axis=0, keepdims=True)\n",
    "    \n",
    "    W1 -= learning_rate * X.T.dot(hidden_layer_gradient)\n",
    "    b1 -= learning_rate * np.sum(hidden_layer_gradient, axis=0, keepdims=True)\n",
    "\n",
    "    # Every 1000 iterations, print the loss\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "# Final output after training\n",
    "print(\"Final output after training:\")\n",
    "print(output_layer_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.38038349310856134\n",
      "Iteration 1000, Loss: 0.24997670387612728\n",
      "Iteration 2000, Loss: 0.24972913712906816\n",
      "Iteration 3000, Loss: 0.24883556540840868\n",
      "Iteration 4000, Loss: 0.2430179563469606\n",
      "Iteration 5000, Loss: 0.2023128270301176\n",
      "Iteration 6000, Loss: 0.1342851241018072\n",
      "Iteration 7000, Loss: 0.03423820536202099\n",
      "Iteration 8000, Loss: 0.012399204540152104\n",
      "Iteration 9000, Loss: 0.006836030803331477\n",
      "Final output after training:\n",
      "[[0.07125372]\n",
      " [0.93446125]\n",
      " [0.93473281]\n",
      " [0.06774813]]\n"
     ]
    }
   ],
   "source": [
    "# Code block to implement backpropagation in Python\n",
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Neural Network structure\n",
    "input_layer_size = 3\n",
    "hidden_layer_size = 4\n",
    "output_layer_size = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "W1 = np.random.rand(input_layer_size, hidden_layer_size)\n",
    "b1 = np.random.rand(1, hidden_layer_size)\n",
    "W2 = np.random.rand(hidden_layer_size, output_layer_size)\n",
    "b2 = np.random.rand(1, output_layer_size)\n",
    "\n",
    "# Input data and target output\n",
    "X = np.array([[0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1]])  # 4 examples, 3 features\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR problem\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "iterations = 10000\n",
    "\n",
    "# Training loop\n",
    "for i in range(iterations):\n",
    "    # Forward pass\n",
    "    hidden_layer_input = np.dot(X, W1) + b1\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    \n",
    "    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = np.mean((output_layer_output - y) ** 2)\n",
    "    \n",
    "    # Backward pass\n",
    "    output_layer_error = output_layer_output - y\n",
    "    output_layer_gradient = output_layer_error * sigmoid_derivative(output_layer_output)\n",
    "    \n",
    "    hidden_layer_error = output_layer_gradient.dot(W2.T)\n",
    "    hidden_layer_gradient = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # Update weights and biases using gradient descent\n",
    "    W2 -= learning_rate * hidden_layer_output.T.dot(output_layer_gradient)\n",
    "    b2 -= learning_rate * np.sum(output_layer_gradient, axis=0, keepdims=True)\n",
    "    \n",
    "    W1 -= learning_rate * X.T.dot(hidden_layer_gradient)\n",
    "    b1 -= learning_rate * np.sum(hidden_layer_gradient, axis=0, keepdims=True)\n",
    "\n",
    "    # Every 1000 iterations, print the loss\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "# Final output after training\n",
    "print(\"Final output after training:\")\n",
    "print(output_layer_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
