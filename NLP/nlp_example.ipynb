{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Detection Project Using TensorFlow\n",
    "\n",
    "This project will go through:\n",
    "\n",
    "1. Data cleaning and preprocessing (including tokenization).\n",
    "2. Text vectorization with TensorFlowâ€™s TextVectorization layer.\n",
    "3. Building, training, and evaluating a simple neural network model for spam detection.\n",
    "\n",
    "##### Project Setup\n",
    "**Step 1: Install TensorFlow**\n",
    "\n",
    "Ensure you have TensorFlow installed:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample dataset (mock data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tf.constant([\n",
    "    \"Congratulations! You've won a free ticket to the Bahamas. Call now!\",\n",
    "    \"Hey, could you meet me at the coffee shop tomorrow?\",\n",
    "    \"Limited time offer! Get 50% off on all items. Buy now!\",\n",
    "    \"Are you free to join our team meeting later today?\",\n",
    "    \"Important notice: Your account has been compromised. Verify now!\",\n",
    "    \"Let's catch up soon. How about lunch next week?\"\n",
    "])\n",
    "\n",
    "# Labels (1 for spam, 0 for not spam)\n",
    "labels = tf.constant([1, 0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text function: lowercasing and removing punctuation\n",
    "def clean_text(text):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, r'[^\\w\\s]', '')  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function\n",
    "texts = clean_text(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words manually (optional)\n",
    "stop_words = [\"a\", \"the\", \"is\", \"to\", \"and\", \"in\", \"of\", \"on\", \"for\", \"now\"]\n",
    "\n",
    "# Custom standardization function to remove stop words\n",
    "def custom_standardization(input_text):\n",
    "    lowercase_text = tf.strings.lower(input_text)\n",
    "    cleaned_text = tf.strings.regex_replace(lowercase_text, r'[^\\w\\s]', '')  # Remove punctuation\n",
    "    for word in stop_words:\n",
    "        cleaned_text = tf.strings.regex_replace(cleaned_text, r'\\b' + word + r'\\b', '')\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TextVectorization layer\n",
    "max_tokens = 1000  # Maximum vocabulary size\n",
    "output_sequence_length = 20  # Maximum number of words per message\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=output_sequence_length,\n",
    "    standardize=custom_standardization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the vectorization layer to our texts\n",
    "vectorize_layer.adapt(texts)\n",
    "\n",
    "# Vectorize the texts\n",
    "vectorized_texts = vectorize_layer(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Texts: tf.Tensor(\n",
      "[[37  4  6  3 13 44 41  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [33 36  2 23 24 45 39 16 10  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [26 12 18 35 50 19 47 30 42  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [46  2  3 29 17 14 22 28 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [31 20  5 48 34 43 38  8  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [27 40  9 15 32 49 25 21  7  0  0  0  0  0  0  0  0  0  0  0]], shape=(6, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorized Texts:\", vectorized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=16, input_length=output_sequence_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 16)            16000     \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16289 (63.63 KB)\n",
      "Trainable params: 16289 (63.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 3s - loss: 0.6927 - accuracy: 0.5000 - 3s/epoch - 3s/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 0.6919 - accuracy: 0.5000 - 14ms/epoch - 14ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 0.6912 - accuracy: 0.5000 - 16ms/epoch - 16ms/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 0.6905 - accuracy: 0.5000 - 18ms/epoch - 18ms/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 0.6898 - accuracy: 0.5000 - 17ms/epoch - 17ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 0.6891 - accuracy: 0.5000 - 17ms/epoch - 17ms/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.6883 - accuracy: 0.5000 - 9ms/epoch - 9ms/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.6876 - accuracy: 0.5000 - 13ms/epoch - 13ms/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.6868 - accuracy: 0.6667 - 17ms/epoch - 17ms/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.6860 - accuracy: 0.6667 - 15ms/epoch - 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the vectorized texts\n",
    "history = model.fit(vectorized_texts, labels, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 694ms/step - loss: 0.6852 - accuracy: 1.0000\n",
      "Model accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(vectorized_texts, labels)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    "\n",
    "# Test the model with new samples\n",
    "new_texts = tf.constant([\n",
    "    \"Exclusive offer! Act fast to claim your free reward.\",\n",
    "    \"Hi, just wanted to check in and see how you are doing.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 278ms/step\n",
      "Predictions: [[0.5074568 ]\n",
      " [0.50704896]]\n"
     ]
    }
   ],
   "source": [
    "# Clean and vectorize new texts\n",
    "new_texts = clean_text(new_texts)\n",
    "vectorized_new_texts = vectorize_layer(new_texts)\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(vectorized_new_texts)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
