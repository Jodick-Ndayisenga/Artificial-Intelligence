{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing (NLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "##### **Tokenization**\n",
    "\n",
    "- Tokenization is the process of representing words as sequences of numbers, enabling computers to process and understand text. Using TensorFlow's Keras API to tokenize sentences, it creates a dictionary mapping words to their corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **sequences**\n",
    "\n",
    "- Sequences are lists of numbers that represent the order of words in a sentence or document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the the tokenizer has all the sentences words, we can see the length is well kept and words are correctly ordered.\n",
    "\n",
    "However, sometimes we might have words that the tokenizer doesn't recognize.\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "new_sentence = [\n",
    "    'I really love my dog',\n",
    "    'My dog loves my manatee'\n",
    "]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentence)\n",
    "print(word_index)\n",
    "print(new_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from these sentences we can see that the length of our sentences changes and some words such as `really`, `loves` and `manatee` that we didn't know about were removed. To make sure that the length of our sentences is at least kept, we use a concept called padding `OOV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)\n",
    "new_sentences = tokenizer.texts_to_sequences(\n",
    "    new_sentence\n",
    ")\n",
    "\n",
    "print(new_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the model to handle sentences of different lengths we will then implement padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  4  2  1  3]\n",
      " [ 0  0  0  4  2  1  6]\n",
      " [ 0  0  0  5  2  1  3]\n",
      " [ 7  5  8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
